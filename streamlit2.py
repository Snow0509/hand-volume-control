import cv2
import mediapipe as mp
import numpy as np
import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration
import av
import math
import logging

# åˆå§‹åŒ–æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)

# WebRTC é…ç½®
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

class HandGestureTransformer(VideoTransformerBase):
    def __init__(self):
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            max_num_hands=1,
            model_complexity=0,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.7
        )
        self.mp_draw = mp.solutions.drawing_utils
        self.tip_ids = [4, 8, 12, 16, 20]  # æ‹‡æŒ‡å’Œå››æŒ‡æŒ‡å°–çš„å…³é”®ç‚¹ç´¢å¼•
        self.vol_history = []
        self.smooth_factor = 5
        self.min_dist = 30  # æœ€å°è·ç¦»ï¼ˆé™éŸ³ï¼‰
        self.max_dist = 300  # æœ€å¤§è·ç¦»ï¼ˆ100%éŸ³é‡ï¼‰

    def transform(self, frame):
        try:
            img = frame.to_ndarray(format="bgr24")
            img = cv2.resize(img, (640, 480))  # é™ä½åˆ†è¾¨ç‡æé«˜æ€§èƒ½
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            results = self.hands.process(img_rgb)

            if results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    self.mp_draw.draw_landmarks(
                        img, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)

                    # è·å–å…³é”®ç‚¹åæ ‡
                    lm_list = []
                    for id, lm in enumerate(hand_landmarks.landmark):
                        h, w, c = img.shape
                        cx, cy = int(lm.x * w), int(lm.y * h)
                        lm_list.append([id, cx, cy])

                    # è®¡ç®—æ‹‡æŒ‡å’Œé£ŸæŒ‡è·ç¦»
                    if len(lm_list) >= 9:
                        x1, y1 = lm_list[4][1], lm_list[4][2]  # æ‹‡æŒ‡æŒ‡å°–
                        x2, y2 = lm_list[8][1], lm_list[8][2]  # é£ŸæŒ‡æŒ‡å°–
                        length = math.hypot(x2 - x1, y2 - y1)

                        # å¹³æ»‘å¤„ç†
                        vol = np.interp(length, [self.min_dist, self.max_dist], [0, 100])
                        self.vol_history.append(vol)
                        if len(self.vol_history) > self.smooth_factor:
                            self.vol_history.pop(0)
                        smooth_vol = sum(self.vol_history) / len(self.vol_history)

                        # è®¾ç½®æµè§ˆå™¨éŸ³é‡
                        self._set_browser_volume(smooth_vol)

                        # ç»˜åˆ¶UI
                        self._draw_ui(img, smooth_vol, x1, y1, x2, y2)

            return img
        except Exception as e:
            logging.error(f"è§†é¢‘å¤„ç†é”™è¯¯: {str(e)}")
            return frame

    def _draw_ui(self, img, vol_percent, x1, y1, x2, y2):
        """ç»˜åˆ¶æ‰‹åŠ¿çº¿å’ŒéŸ³é‡æ¡"""
        # è¿æ¥æ‹‡æŒ‡å’Œé£ŸæŒ‡çš„çº¿
        cv2.line(img, (x1, y1), (x2, y2), (255, 0, 255), 2)
        cv2.circle(img, (x1, y1), 8, (255, 0, 255), cv2.FILLED)
        cv2.circle(img, (x2, y2), 8, (255, 0, 255), cv2.FILLED)

        # æ˜¾ç¤ºéŸ³é‡ç™¾åˆ†æ¯”
        cv2.putText(img, f'Volume: {int(vol_percent)}%', (20, 40),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        # ç»˜åˆ¶éŸ³é‡æ¡
        bar_width, bar_height = 25, 150
        bar_x, bar_y = 20, 60
        vol_fill_height = int(np.interp(vol_percent, [0, 100], [0, bar_height]))

        cv2.rectangle(img, (bar_x, bar_y),
                     (bar_x + bar_width, bar_y + bar_height),
                     (0, 255, 0), 1)
        cv2.rectangle(img, (bar_x, bar_y + bar_height - vol_fill_height),
                     (bar_x + bar_width, bar_y + bar_height),
                     (0, 255, 0), cv2.FILLED)

    def _set_browser_volume(self, volume_percent):
        """é€šè¿‡JavaScriptæ§åˆ¶æµè§ˆå™¨éŸ³é‡"""
        js_code = f"""
        <script>
        const audioElements = document.getElementsByTagName('audio');
        for (let audio of audioElements) {{
            audio.volume = {volume_percent / 100};
        }}
        </script>
        """
        st.components.v1.html(js_code, height=0)

def main():
    st.title("æ‰‹åŠ¿æ§åˆ¶ç½‘é¡µéŸ³é‡")
    st.markdown("""
    ### ä½¿ç”¨è¯´æ˜ï¼š
    1. å…è®¸æµè§ˆå™¨è®¿é—®æ‘„åƒå¤´ã€‚
    2. æ‰‹åŠ¿æ§åˆ¶ï¼š
       - ğŸ‘† æ‹‡æŒ‡å’Œé£ŸæŒ‡åˆ†å¼€ï¼šå¢å¤§éŸ³é‡
       - ğŸ¤ æ‹‡æŒ‡å’Œé£ŸæŒ‡é è¿‘ï¼šå‡å°éŸ³é‡
    *æ³¨æ„ï¼šæ­¤åŠŸèƒ½ä»…æ§åˆ¶å½“å‰ç½‘é¡µçš„éŸ³é‡ï¼Œä¸å½±å“ç³»ç»ŸéŸ³é‡ã€‚*
    """)

    # åˆå§‹åŒ–WebRTCæµ
    webrtc_ctx = webrtc_streamer(
        key="hand-gesture",
        mode=WebRtcMode.SENDRECV,
        rtc_configuration=RTC_CONFIGURATION,
        video_processor_factory=HandGestureTransformer,
        media_stream_constraints={"video": True, "audio": True},  # å¯ç”¨éŸ³é¢‘
        async_processing=True,
    )

    if st.button("åœæ­¢åº”ç”¨"):
        st.experimental_rerun()

if __name__ == "__main__":
    main()